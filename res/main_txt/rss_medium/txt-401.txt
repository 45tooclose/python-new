
Forex USDCAD Price Predict Every 15 minutesSince I spend my most of working hours on motion sensor and image data, I thought it’d be fun to dig around some other interesting areas to apply my skills. Then I thought: “Hey, there are tons of easily accessible data around stock and foreign exchange rate. Let’s see if I can make something useful out of them.”I’ve never had any training in financial sector, other than learning how to file my taxes (but I gave up learning it after a few hours and decided to delegate that task to my accountant). So, the following is the result I’ve achieved through some trials and errors over a few weekends:Photo by Christine Roy on UnsplashI am a firm believer in KISS (Keep it simple, stupid) principle. I’ve tried my best to keep the code short and sweet. If anything is unclear, let me know and I will try my best to explain it.OutlinesObtaining DataFeature ExplorationShaping DataTrainingTestingPostmortemObtaining DataThe first thing I did was to obtain the Oanda. There is a Python library that encapsulates Oanda’s REST API v2 that was easy to deal with. For my model, I used historical USDCAD rates since Jan 1. 2012 (about <6 years worth) which I think, was a decent size.Let the coding beginFirst import libraries.# In my case, I was using Keras to build the models with TensorFlow backend with GPU supportfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Activation, Input, LSTM, Dense, merge, Flattenfrom keras.models import load_modelfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.metrics import mean_squared_errorfrom sklearn.decomposition import PCA, KernelPCAimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns#Used TA-Lib for creating additional features. More on this later.from talib.abstract import *from talib import MA_TypeConfiguration variables:TARGET     = 'USD_CAD'granuality = 'H1'LOOK_BACK  = 20SPLIT      = 0.99 # data split ration for training and testingBelow is what the data look like:DATA.head()Feature ExplorationAdding more features which helped improving the accuracy. Also, by adding hour, day, and week, I think it helped with mitigating a seasonality problem.# Compute various features that were not available in the raw dataDATA['hour'] = DATA.index.hourDATA['day']  = DATA.index.weekdayDATA['week'] = DATA.index.weekDATA['volume'] = pd.to_numeric(DATA['volume'])DATA['close']  = pd.to_numeric(DATA['close'])DATA['open']   = pd.to_numeric(DATA['open'])DATA['momentum']   = DATA['volume'] * (DATA['open'] - DATA['close'])DATA['avg_price']  = (DATA['low'] + DATA['high'])/2DATA['range']      = DATA['high'] - DATA['low']DATA['ohlc_price'] = (DATA['low'] + DATA['high'] + DATA['open'] + DATA['close'])/4DATA['oc_diff']      = DATA['open'] - DATA['close']DATA['spread_open']  = DATA['ask_open'] - DATA['bid_open']DATA['spread_close'] = DATA['ask_close'] - DATA['bid_close']inputs = {    'open'   : DATA['open'].values,    'high'   : DATA['high'].values,    'low'    : DATA['low'].values,    'close'  : DATA['close'].values,    'volume' : DATA['volume'].values}DATA['ema'] = MA(inputs, timeperiod=15, matype=MA_Type.T3)DATA['bear_power'] = DATA['low'] - DATA['ema']DATA['bull_power'] = DATA['high'] - DATA['ema']# Since computing EMA leave some of the rows empty, we want to remove them. (EMA is a lagging indicator)DATA.dropna(inplace=True)# Add 1D PCA vector as a feature as well. This helped increasing the accuracy by adding more variance to the feature setpca_input = DATA.drop('close').copy()pca_features = pca_input.columns.tolist()pca = PCA(n_components=1)DATA['pca'] = pca.fit_transform(pca_input.values.astype('float32'))Note: I’ve used TA-Lib to compute EMA then computed Bears Power, and Bulls Power as features.What my data looked like after adding all the features:Let’s verify that bull_power and bear_power adds some values by creating more variance in feature sets, we want to check the correlation heat map between these features:t = DATA[['close', 'bull_power', 'bear_power']].copy()t['pct_change'] = t['close'].pct_change()t.dropna(inplace=True)corr =  t.corr()mask = np.zeros_like(corr, dtype=np.bool)mask[np.triu_indices_from(mask)] = Truef, ax = plt.subplots(figsize=(5, 5))cmap = sns.diverging_palette(220, 10, as_cmap=True)sns.heatmap(corr, mask=mask, cmap=cmap, ax=ax)Correlation plot. pct_change (Percentage change in CLOSE price) row is what we want to check.Okay, there was some degree of correlation which can be useful.Let’s check PCA feature vector:import matplotlib.colors as colorsimport matplotlib.cm as cmimport pylabnorm = colors.Normalize(DATA['high'].values.min(), DATA['high'].values.max())color = cm.viridis(norm(DATA['high'].values))for col in DATA.columns.tolist():    if col != 'pca':        plt.figure(figsize=(10,5))        plt.scatter(DATA[col].values, DATA['pca'].values, lw=0, c=color, cmap=pylab.cm.cool, alpha=0.3, s=1)        plt.title(col + ' vs pca')        plt.show()There was to be some degree of seperation in PCA plot. It wouldn’t hurt to utilize it.Overall correlation pair plots(This was pretty useless and took a very long time to generate, but thought it looked cool)